{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87c28c-7c1a-44a2-9be8-e5b78677fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc145595-a4bf-425f-b918-8c00f8176f52",
   "metadata": {},
   "source": [
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "    * Lowercase everything\n",
    "    * Normalize unicode characters\n",
    "    * Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671cb44-2e09-4751-a112-d410b855c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    \"\"\"\n",
    "    Apply basic text cleaning to the input string.\n",
    "    \n",
    "    This function performs the following steps:\n",
    "    - Lowercase the text.\n",
    "    - Normalize unicode characters.\n",
    "    - Replace characters that are not letters, numbers, whitespace, or single quotes.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: The input text to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "    Cleaned text after applying the basic cleaning operations.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    #Replace any characters that are not letters, numbers, spaces, or single quotes.\n",
    "\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99e35d-c25c-4cf3-9543-f76430e02c77",
   "metadata": {},
   "source": [
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7b7da-aefa-451e-853b-40e7c7488ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize words in the input string.\n",
    "    \n",
    "    This function tokenizes the input text into individual words.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: The input text to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    A list of tokenized words.\n",
    "    \"\"\"\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db8de0-fcc7-4ece-a867-6e479a588750",
   "metadata": {},
   "source": [
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d189be-0ee4-4ee6-a847-1e2254a59dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    \"\"\"\n",
    "    Apply stemming to words in the input text.\n",
    "    \n",
    "    This function applies stemming to each word in the input text using the Porter Stemmer algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: The input text to apply stemming to.\n",
    "    \n",
    "    Returns:\n",
    "    Text with words after stemming.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in nltk.word_tokenize(text)]\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a888d2-f5e3-4b55-9c60-79a7a70a2561",
   "metadata": {},
   "source": [
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6629447-d55c-462c-b3d5-ca0ef1ced725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    Apply lemmatization to words in the input text.\n",
    "    \n",
    "    This function applies lemmatization to each word in the input text using the WordNet Lemmatizer.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: The input text to apply lemmatization to.\n",
    "    \n",
    "    Returns:\n",
    "    Text with words after lemmatization.\n",
    "    \"\"\"\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3beaf-1f01-4724-bb29-fbfc1a7d538d",
   "metadata": {},
   "source": [
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords. ",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51709b-406a-404f-8942-437c6a8cb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, extra_words=None, exclude_words=None):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the input text.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text to remove stopwords from.\n",
    "    - extra_words (list): List of additional words to include as stopwords.\n",
    "    - exclude_words (list): List of words not to be removed.\n",
    "    \n",
    "    Returns:\n",
    "    str: Text with stopwords removed.\n",
    "    \"\"\"\n",
    "    # Load the stopwords list from the NLTK library\n",
    "    stopword_list = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add extra words to the stopwords list if provided\n",
    "    if extra_words:\n",
    "        stopword_list.update(extra_words)\n",
    "        \n",
    "    # Remove excluded words from the stopwords list if provided\n",
    "    if exclude_words:\n",
    "        stopword_list.difference_update(exclude_words)\n",
    "        \n",
    "    # Tokenize the input text into individual words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Filter out words that are in the stopwords list\n",
    "    filtered_words = [word for word in words if word.lower() not in stopword_list]\n",
    "    \n",
    "    # Reconstruct the filtered words into a text string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7e144-73ba-4918-8369-c2b22834ad7b",
   "metadata": {},
   "source": [
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c37c58-e5a2-4f79-ab3c-8e8be89a8d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "287774fc-2bf3-42f1-9ff6-0a321ac79be6",
   "metadata": {},
   "source": [
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb041f97-0387-4c89-88a2-6e7294bd12be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "005f5816-05b2-4bcd-b698-919d6176f168",
   "metadata": {},
   "source": [
    "8. For each dataframe, produce the following columns:\n",
    "    * title to hold the title\n",
    "    * original to hold the original article/post content\n",
    "    * clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "    * stemmed to hold the stemmed version of the cleaned data.\n",
    "    * lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470af95-8152-4a1c-a8c0-6fa5d006b7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a78c7d0a-4f6e-4d7f-877a-ea9169ab698a",
   "metadata": {},
   "source": [
    "9. Ask yourself:\n",
    "    * If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "    * If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "    * If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175aac2-f145-4320-8ac5-448792a6375c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
